{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23736d09",
   "metadata": {},
   "source": [
    "## BitsandBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24584ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 5 for processing.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "import os\n",
    "def gpu_info():\n",
    "    dat=[]\n",
    "    import subprocess\n",
    "    out = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "    out2 = out.split(\"\\n\")[8:]\n",
    "    splitter = out2[3]\n",
    "    out3 = \"\\n\".join(out2).split(splitter)[:-1]\n",
    "\n",
    "    # Check GPU usage\n",
    "    for idx,x in enumerate(out3):\n",
    "        gpu = ((x.split(\"|\"))[1].replace(\"Off\", \"\"))\n",
    "        t = ((x.split(\"|\"))[6].replace(\"Off\", \"\").replace(\"MiB\", \"\").split(\"/\"))\n",
    "        ans = (int(t[0]) / int(t[1])) * 100\n",
    "        dat.append({\n",
    "                \"gpu_id\": idx,\n",
    "                \"gpu\": ((gpu[:-5])[6:]).strip(),\n",
    "                \"usage\": f\"{ans:.2f}\"\n",
    "            })\n",
    "    return dat\n",
    "gpu_info_ans = gpu_info()\n",
    "flag=0\n",
    "for gpu in gpu_info_ans:\n",
    "    if float(gpu['usage'])<10: \n",
    "        empty_gpu = gpu['gpu_id']\n",
    "        flag=1\n",
    "        break\n",
    "if flag==0:\n",
    "    print(\"No empty GPU found, please check your GPU usage.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"Using GPU {empty_gpu} for processing.\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(empty_gpu)\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mshahidul/miniconda3/envs/web/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # reduces memory usage (by ~8x compared to FP32).\n",
    "    bnb_4bit_compute_dtype=torch.float16, # perform internal math operations (like attention, matrix multiplications) in float16 precision.\n",
    "    bnb_4bit_use_double_quant=True, # use double quantization to reduce memory usage further.(8bit then 4bit)\n",
    "    bnb_4bit_quant_type=\"nf4\", # NormalFloat4 â€” designed to retain more information with fewer bits\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-0.5B-Instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\" # automatically assigns the model to the available GPU(s) based on the environment variable CUDA_VISIBLE_DEVICES\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4772a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3660da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum computing in simple terms. Quantum computing is a type of computing that uses quantum mechanics to process information, much like classical computers do.\n",
      "\n",
      "In classical computing, data is represented by bits (0s and 1s), which can be stored and processed individually. In quantum computing, data is represented as quantum bits or qubits. When a qubit is measured, it collapses into one of two states - 0 or 1. This means that the same amount of information can be stored using more qubits than just one qubit. By doing this, quantum computers can perform many calculations at once, allowing them to solve problems that are beyond the capabilities of classical computers.\n",
      "\n",
      "There are some key features of quantum computing that make it different from classical computing:\n",
      "\n",
      "- Superposition: A\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output_tokens = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Avoid warning for missing pad token\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c934d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
