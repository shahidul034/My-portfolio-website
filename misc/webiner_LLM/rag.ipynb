{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85475a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/mshahidul/'))\n",
    "from gpu_selection import _gpu_selection_\n",
    "_gpu_selection_()\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(file_path=\"/home/mshahidul/webiner/ML.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split text\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Embedding model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "# Save to Chroma vector store\n",
    "chroma_dir = \"langchain_pyloader/chroma_db\"\n",
    "vectorstore = Chroma.from_documents(docs, hf, persist_directory=chroma_dir)\n",
    "vectorstore.persist()\n",
    "\n",
    "# Load vector store later\n",
    "new_vectorstore = Chroma(persist_directory=chroma_dir, embedding_function=hf)\n",
    "\n",
    "print(\"PDF read and vectorized using Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "# --- 1. Load the embedding function ---\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# --- 2. Load Chroma vectorstore from disk ---\n",
    "chroma_dir = \"langchain_pyloader/chroma_db\"\n",
    "vectorstore = Chroma(persist_directory=chroma_dir, embedding_function=embedding_model)\n",
    "\n",
    "# --- 3. Set up retriever ---\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f18e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "with open('/home/mshahidul/api.json', 'r') as f_api:\n",
    "    api_data = json.load(f_api)\n",
    "# generator = OpenAI(api_key=api_data[\"openai_api_key\"])\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=api_data[\"openai_api_key\"],\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name=\"unsloth/Qwen2.5-14B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# nf4_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_use_double_quant=True,\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model_nf4 = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=nf4_config,device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186dcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_nf4,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.3,\n",
    "#     top_p=0.9\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# --- 5. Build RetrievalQA chain ---\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf1c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 4. Load HuggingFace model (Mistral 7B as example) ---\n",
    "# model_name = \"unsloth/Qwen2.5-0.5B-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",               # automatically selects GPU/CPU\n",
    "#     torch_dtype=torch.float16        # use float32 if running on CPU\n",
    "# )\n",
    "\n",
    "# generator = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_nf4,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_new_tokens=256,\n",
    "#     temperature=0.3,\n",
    "#     top_p=0.9\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=generator)\n",
    "\n",
    "# # --- 5. Build RetrievalQA chain ---\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm,\n",
    "#     retriever=retriever,\n",
    "#     return_source_documents=True\n",
    "# )\n",
    "\n",
    "# --- 6. Ask questions! ---\n",
    "query = \"What is svm?\"\n",
    "result = qa_chain({\"query\": query})\n",
    "\n",
    "# --- 7. Display result ---\n",
    "print(\"Answer:\\n\", result[\"result\"])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Page: {doc.metadata.get('page', 'N/A')} | Content Snippet: {doc.page_content[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a8161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query):\n",
    "    result = qa_chain({\"query\": query})\n",
    "    answer = result[\"result\"]\n",
    "    sources = \"\\n\".join(\n",
    "        f\"Page: {doc.metadata.get('page', 'N/A')} - {doc.page_content[:100]}...\"\n",
    "        for doc in result[\"source_documents\"]\n",
    "    )\n",
    "    return answer, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c1a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gradio Interface ---\n",
    "import gradio as gr\n",
    "iface = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ask something about the PDF...\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Answer\"),\n",
    "        gr.Textbox(label=\"Source Chunks\")\n",
    "    ],\n",
    "    title=\"PDF QA (RAG) with Chroma + Qwen\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fecbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
