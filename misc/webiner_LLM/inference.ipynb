{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "586b37cb",
   "metadata": {},
   "source": [
    "## Inference using unsloth ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97e61c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 2 for processing.\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "import os\n",
    "def gpu_info():\n",
    "    dat=[]\n",
    "    import subprocess\n",
    "    out = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True).stdout\n",
    "    out2 = out.split(\"\\n\")[8:]\n",
    "    splitter = out2[3]\n",
    "    out3 = \"\\n\".join(out2).split(splitter)[:-1]\n",
    "\n",
    "    # Check GPU usage\n",
    "    for idx,x in enumerate(out3):\n",
    "        gpu = ((x.split(\"|\"))[1].replace(\"Off\", \"\"))\n",
    "        t = ((x.split(\"|\"))[6].replace(\"Off\", \"\").replace(\"MiB\", \"\").split(\"/\"))\n",
    "        ans = (int(t[0]) / int(t[1])) * 100\n",
    "        dat.append({\n",
    "                \"gpu_id\": idx,\n",
    "                \"gpu\": ((gpu[:-5])[6:]).strip(),\n",
    "                \"usage\": f\"{ans:.2f}\"\n",
    "            })\n",
    "    return dat\n",
    "gpu_info_ans = gpu_info()\n",
    "flag=0\n",
    "for gpu in gpu_info_ans:\n",
    "    if float(gpu['usage'])<10: \n",
    "        empty_gpu = gpu['gpu_id']\n",
    "        flag=1\n",
    "        break\n",
    "if flag==0:\n",
    "    print(\"No empty GPU found, please check your GPU usage.\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(f\"Using GPU {empty_gpu} for processing.\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(empty_gpu)\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b50662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "model_name = \"/home/mshahidul/project1/model/unsloth/Qwen2.5-14B-Instruct_alpaca\"\n",
    "# model_name = \"Qwen2.5-14B-Instruct\"\n",
    "max_seq_length,dtype,load_in_4bit,model_name = 4092,None ,True, \n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name, # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a62322",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "                    ### Instruction:\n",
    "                    {}\n",
    "\n",
    "                    ### Input:\n",
    "                    {}\n",
    "\n",
    "                    ### Response:\n",
    "                    {}\"\"\"\n",
    "prompt='Translate the following English text into Spanish naturally and accurately:'\n",
    "ques = \"The cat is on the roof.\"\n",
    "inputs = tokenizer(\n",
    "    [\n",
    "        alpaca_prompt.format(\n",
    "            f\"{prompt}\", # instruction\n",
    "            ques, # input\n",
    "            \"\", # output - leave this blank for generation!\n",
    "        )\n",
    "    ], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, temperature = 0.3, top_p = 0.9, top_k = 32,do_sample=True)\n",
    "ans=tokenizer.batch_decode(outputs)\n",
    "start_marker = '### Response:\\n'\n",
    "end_marker = '<|im_end|'\n",
    "start_index = ans[0].find(start_marker) + len(start_marker)\n",
    "end_index = ans[0].find(end_marker)\n",
    "response = ans[0][start_index:end_index].strip()\n",
    "print(f\"Input: {ques}\")\n",
    "print(f\"Output: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b217b",
   "metadata": {},
   "source": [
    "## vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/mshahidul/'))\n",
    "from gpu_selection import _gpu_selection_\n",
    "_gpu_selection_()\n",
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "llm = LLM(model=\"unsloth/Qwen2.5-14B-Instruct\",dtype=torch.float16,quantization=\"bitsandbytes\", load_format=\"bitsandbytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template= f\"Explain the large language models.\"\n",
    "sampling_params = SamplingParams(temperature=1.0, top_p=0.95,max_tokens=256)\n",
    "outputs = llm.generate(prompt_template, sampling_params)\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    response = output.outputs[0].text\n",
    "    print(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceccb93b",
   "metadata": {},
   "source": [
    "CUDA_VISIBLE_DEVICES=6 python -m vllm.entrypoints.openai.api_server \\\n",
    "    --port 8000 \\\n",
    "    --model unsloth/Qwen2.5-14B-Instruct \\\n",
    "    --dtype float16 \\\n",
    "    --quantization bitsandbytes \\\n",
    "    --load-format bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215af639",
   "metadata": {},
   "source": [
    "## [VLLM OpenAI server](https://docs.vllm.ai/en/v0.8.3/serving/openai_compatible_server.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07b9092",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"token-abc123\",\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"unsloth/Qwen2.5-14B-Instruct\",\n",
    "  messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aba516",
   "metadata": {},
   "source": [
    "wget https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
    "CUDA_VISIBLE_DEVICES=6 vllm serve ./tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n",
    "   --tokenizer TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53154efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/mshahidul/'))\n",
    "from gpu_selection import _gpu_selection_\n",
    "_gpu_selection_()\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# In this script, we demonstrate how to pass input to the chat method:\n",
    "conversation = [\n",
    "   {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": \"You are a helpful assistant\"\n",
    "   },\n",
    "   {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hello\"\n",
    "   },\n",
    "   {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Hello! How can I assist you today?\"\n",
    "   },\n",
    "   {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Write an essay about the importance of higher education.\",\n",
    "   },\n",
    "]\n",
    "\n",
    "# Create a sampling params object.\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
    "\n",
    "# Create an LLM.\n",
    "llm = LLM(model=\"/home/mshahidul/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\",\n",
    "         tokenizer=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "# Generate texts from the prompts. The output is a list of RequestOutput objects\n",
    "# that contain the prompt, generated text, and other information.\n",
    "outputs = llm.chat(conversation, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "   prompt = output.prompt\n",
    "   generated_text = output.outputs[0].text\n",
    "   print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd116a",
   "metadata": {},
   "source": [
    "python /home/mshahidul/webiner/inference.py --finetune True --ques \"The cut through the muscle is then closed horizontally to keep the pylorus open and allow the stomach to empty.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9265436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'The cut through the muscle is then closed horizontally to keep the pylorus open and allow the stomach to empty.',\n",
       " 'spanish': 'Luego, se cierra horizontalmente la incisión a través del músculo para mantener el píloro abierto y permitir que el estómago se vacíe.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/home/mshahidul/project1/all_tran_data/dataset/medline_data_for_finetune.json\", \"r\") as f:\n",
    "    medline_data = json.load(f)\n",
    "\n",
    "medline_data[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719e1b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
