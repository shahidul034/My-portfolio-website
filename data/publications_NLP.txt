

				<li><b>Md. Shahidul Salim</b>, Sk Imran Hossain,
					An Applied Statistics dataset for human vs AI-generated answer classification, Data in Brief, 2024,
					110240, ISSN 2352-3409,https://doi.org/10.1016/j.dib.2024.110240.
					<a href="https://www.sciencedirect.com/science/article/pii/S2352340924002117" target="_blank"
						style="color: #03bafc;">üåêpaper</a>
					<a href="https://github.com/shahidul034/An-Applied-Statistics-Dataset-for-Human-vs-AI-Generated-Answer-Classification"
						target="_blank" style="color: #03bafc;">üîóGithub</a>
					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">We have developed a dataset of 4231
								question-and-answer pairs for
								Applied Statistics, which
								includes 116 questions selected by domain experts and answers provided by 100 students
								and
								50 AI-generated responses using ChatGPT.
								Our dataset can be used to develop AI detectors for the Applied Statistics domain and to
								evaluate existing ones. Additionally, our data collection framework can be applied to
								other
								domains.</i></p>
					</div>
				</li>
				<br>
				<li>Bose, D., & <b>Salim, M. S.</b> (2024). Suggesting Bengali words using masked language model. In 3rd
					international conference on computing
					advancements (ICCA).
					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">This study explores Bangla word suggestion in texts using a
								fine-tuned Bangla-BERT model with Masked Language Modeling (MLM). While
								transformer-based models like BERT have revolutionized natural language processing
								(NLP), the availability of task-specific datasets, especially for word-suggesting in
								Bangla, is limited. This research addresses this gap by employing MLM to predict Bangla
								words. Sub-word tokenization enhances the model's ability to handle unknown words.
								Unlike existing pretrained BanglaBERT model that uses word-piece masking, we experiment
								with dynamic whole-word masking algorithms. Our research aims to analyze the performance
								of the fine-tuned BanglaBERT model under different masking strategies and compare
								results to the traditional BanglaBERT approach.</i></p>
					</div>
				</li>
				<br>
				<li>Asif Mohammed Saad, Umme Niraj Mahi, <b>Md. Shahidul Salim</b>, Sk Imran Hossain,
					Bangla news article dataset,
					Data in Brief,
					2024,
					110874,
					ISSN 2352-3409,
					https://doi.org/10.1016/j.dib.2024.110874.
					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">In this research, we present an updated standard Bangla
								dataset
								based on gathered Bangla news articles.
								In total, more than 1.9 million articles from nine Bangla news websites were gathered;
								the
								selection process was led by a number of categories, including sports, economy,
								politics,
								local news, tech, tourism, entertainment, education, health, the arts, and many more.
								The dataset per newspaper contains varying attributes, such as title, content, time,
								tags,
								meta, category, etc. This dataset will enable data scientists to investigate and assess
								theories related to Bangla natural language processing.
								Furthermore, there is a greater chance that the dataset will be utilized for
								domain-specific
								large language models in the context of Bangladesh, and it may be used to develop deep
								learning and machine learning models that categorize articles according to subjects.
							</i></p>
					</div>
				</li>

				<br>
				<li><b>BCoQA: Benchmark and Resources for Bangla Context-based Conversational Question
						Answering(Submitted to EMNLP 2024(Revision completed))</b>
					<a href="https://openreview.net/pdf?id=uZ4NJWOuwA" target="_blank"
						style="color: #03bafc;">üåêOpenReview</a>

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Developing a Bangla Context-based Conversational Question
								Answering
								(CCQA) system faces challenges like limited domain-specific data, poor translation
								methods,
								and a lack of pretrained language models.
								To tackle these issues, this work constructs a robust Bangla CCQA dataset using
								quality-controlled machine translation and large language model-based augmentation of
								existing English CCQA datasets.
								The dataset is then divided into training, validation, and test splits. Various
								sequence-to-sequence models are fine-tuned and evaluated using the train and test
								splits,
								with conversation history included in the input prompts to maintain context.
							</i></p>
					</div>
				</li>

				<br>
				<li>Nabil, A., Das, d., <b>Salim, M. S.</b>, Arifeen, S., & Fattah, H. M. A. (2023).
					<b>Bangla emergency post classification on social media using transformer based bert models.</b>
					In 6th international conference on electrical information and communication technology (EICT 2023).
					(Accepted)
					<a href="https://drive.google.com/file/d/1i_WAnFPTzB3I_OQ4SQyRsqRi0n3RmvD2/view?usp=sharing"
						target="_blank" style="color: #03bafc;">üåêpaper</a>

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Text classification is one of the most
								important tasks in Natural Language Processing. As text
								data is growing rapidly, it needs more computational
								power to classify the text in a big dataset. The task
								is difficult for characteristic-rich languages like Bangla.
								Having good-quality text data significantly affects the
								outcome of the model that has been used to classify
								them. Nowadays, social media can be an important
								source of information. But there is a huge number of
								data which are of no use. As the use of social media is
								increasing day by day, people are posting about events
								around their surroundings. So, it can be an important
								propaganda of the media. In this study, various text
								classification methods were used to classify the texts
								in Bangla from social media, which can be categorized
								as emergencies that may need immediate actions from
								the government, local authority, or law enforcement or
								even may need international attention. Therefore, 5839
								social media posts were collected from Facebook and
								Twitter, which were written in Bangla along with some
								mixed English words. Then, after preprocessing, various Machine Learning models, Deep
								Neural Network
								models, and Transformer based models were applied to
								classify them. Among these models, transformer-based
								XLM-RoBERTa outperformed all the other models
								with an F1-score of 95.25.</i></p>
					</div>

				</li>
				<br>
				<li><b>Salim, M. S.</b>, Murad, H., Das, D., & Ahmed, F. (2023).
					"<b>BanglaGPT: A Generative Pretrained Transformer-Based Model for Bangla Language</b>,"
					2023 International Conference on Information and Communication Technology for Sustainable
					Development (ICICT4SD), Dhaka, Bangladesh, 2023,
					pp. 56-59, doi: 10.1109/ICICT4SD59951.2023.10303383.<a
						href="https://ieeexplore.ieee.org/document/10303383" target="_blank"
						style="color: #03bafc;">üåêpaper</a>

					<!----Abstract start under hidden div-->

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Natural Language Processing (NLP) has entered
								a new era with the advent
								of pre-trained language models, paving the way for constructing robust language models.
								Pretrained transformer-based models such as GPT-2 have become prevalent due to their
								cutting-edge efficiency. However, these approaches rely heavily on resource-intensive
								languages, forcing other languages to adopt multilingual frameworks (mGPT). The mGPT
								model could perform better for low-resource languages such as Bangla because the model
								has been trained on a diverse dataset spanning multiple languages. Recent studies show
								that the language-specific GPT model outperforms the multilingual mGPT model. In this
								research, we have proposed a pretrained monolingual GPT model called BanglaGPT using the
								objective of causal language modeling (CLM). Due to the lack of available large datasets
								for NLP tasks in Bangla, we have created a Bangla language model dataset called
								BanglaCLM using a 26.24 GB Bangla corpus scraped from several public websites. We have
								used a subword-based tokenization algorithm named Byte-Pair Encoding (BPE) for Bangla
								and finally trained the Bangla-GPT2 model from scratch using the BanglaCLM dataset. Our
								pretrained BanglaGPT provides state-of-the-art performance for Bangla text generation
								with a perplexity score of 2.86 and a loss score of 0.45 on the test set.</i></p>
					</div>

					<!-------End of Abstract Section----------->
				</li>
				<br>
				<li><b>S. Salim</b>, T. Islam, R. Zannat, N. Mia, M. Fuad and H. Murad, "<b>Towards Developing a
						Transformer-Based Bangla Typing Error Correction Model: A Deep Learning-Based Approach</b>,"
					2023 International Conference on Information and Communication Technology for Sustainable
					Development (ICICT4SD), Dhaka, Bangladesh, 2023,
					pp. 75-78, doi: 10.1109/ICICT4SD59951.2023.10303361.<a
						href="https://ieeexplore.ieee.org/abstract/document/10303361" target="_blank"
						style="color: #03bafc;">üåêpaper</a>

					<!----Abstract start under hidden div-->

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Despite being the sixth most widely spoken
								language globally, the
								Bangla language faces a lack of effective and efficient automated methods to correct
								keyboard errors in written text. Such errors can result in communication
								misunderstandings and misinterpretations, particularly in the digital domain, where
								written text plays a crucial role in communication and is heavily relied upon. However,
								no keyboard error model for the Bangla language has been found due to the lack of
								available datasets. In this research work, we have proposed a probabilistic approach to
								generate a synthetic Bangla keyboard typing error correction (BKTEC) dataset. By
								utilizing the novel BKTEC dataset, we have fine-tuned a transformer-based model for
								Keyboard error correction. After training it to convergence, the proposed model was
								tested on a test dataset, and we discovered a word level accuracy of 88.56%.</i></p>
					</div>

					<!-------End of Abstract Section----------->

				</li>
				<br>
				<li>T. Ahmed, S. Hossain, <b>M. S. Salim</b>, A. Anjum and K. M. Azharul Hasan, "<b>Gold Dataset for the
						Evaluation of Bangla Stemmer</b>," 2021 5th International Conference on Electrical Information
					and Communication Technology (EICT), Khulna, Bangladesh, 2021, pp. 1-6,
					doi:10.1109/EICT54103.2021.9733662.<a href="https://ieeexplore.ieee.org/document/9733662"
						target="_blank" style="color: #03bafc;">üåêpaper</a>

					<!----Abstract start under hidden div-->

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Stemming is a preprocessing task for Natural
								Language Processing(NLP)
								that involves normalizing inflected words representing the same concept of the original
								word. Steaming is a process of text normalization that transforms a slated word into its
								root form. It has a great impact in different applications in NLP and Natural Language
								Understanding. One of the biggest challenges in Bangla stemming is to collect the rules
								that are associated with Bangla word stemming and standard dataset for testing the
								accuracy. This paper presents a summery of rules associated with Bangla stemming and a
								gold standard dataset corpus which will helps to testify the stemming algorithm. We
								verify the rules and using our rules and corpus we tested with existing Bangla stemming
								algorithms and found that the proposed corpus makes a difference with existing
								techniques.</i></p>
					</div>

					<!-------End of Abstract Section----------->

				</li>
				<br>
				<li><b>M. S. Salim Shakib</b>, T. Ahmed and K. M. Azharul Hasan, "<b>Designing a Bangla Stemmer using
						rule
						based
						approach</b>," 2019 International Conference on Bangla Speech and Language Processing
					(ICBSLP), Sylhet, Bangladesh, 2019, pp. 1-4, doi: 10.1109/ICBSLP47725.2019.201533.<a
						href="https://ieeexplore.ieee.org/document/9084043" target="_blank"
						style="color: #03bafc;">üåêpaper</a>
					<a href="https://github.com/shahidul034/Designing-a-Bangla-Stemmer-using-rule-based-approach"
						target="_blank" style="color: #03bafc;">üîóGithub</a>


					<!----Abstract start under hidden div-->

					<div class="abstract" style="display: none;">
						<p> üéØ <i style="color: cadetblue;">Stemming is a preprocessing task for natural
								language processing that
								involves normalizing inflected words representing the same concept of the original word.
								Steaming is a process of text normalization that has many applications. There are many
								techniques for steaming of inflected words for different languages but very few works
								for Bangla word steaming. Therefore, stemming Bangla word is a unsolved problem. There
								are many different situations that can occur in Bangla language for word steaming. In
								this paper, we present a rule based algorithm to stem Bangla words. We developed the
								rules for infection detection for verb inflection, number inflection, and others. Using
								our rules, we developed a system to find the root word of Bangla words and found good
								performance. Sufficient examples are provided to explain the proposed system.</i></p>
					</div>

					<!-------End of Abstract Section----------->

				</li>
			